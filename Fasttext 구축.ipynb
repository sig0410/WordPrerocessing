{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 865 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=0.7.0 in /Users/Moon/opt/anaconda3/lib/python3.7/site-packages (from fasttext) (50.3.0)\n",
      "Requirement already satisfied: numpy in /Users/Moon/opt/anaconda3/lib/python3.7/site-packages (from fasttext) (1.18.5)\n",
      "Collecting pybind11>=2.2\n",
      "  Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-macosx_10_15_x86_64.whl size=328496 sha256=67ed9931e2eaaeb5a4c4e6e130680084557ed1610dd4f17a3384097215219bd6\n",
      "  Stored in directory: /Users/Moon/Library/Caches/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PiP\n",
    "1. $ git clone https://github.com/facebookresearch/fastText.git\n",
    "\n",
    "2. $ cd fastText\n",
    "\n",
    "3. $ conda install -c conda-forge fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맥 설치 \n",
    "1. $ xcode-select --install\n",
    "\n",
    "2. $ brew install cmake\n",
    "\n",
    "3. $ brew install gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: fasttext <command> <args>\r\n",
      "\r\n",
      "The commands supported by fasttext are:\r\n",
      "\r\n",
      "  supervised              train a supervised classifier\r\n",
      "  quantize                quantize a model to reduce the memory usage\r\n",
      "  test                    evaluate a supervised classifier\r\n",
      "  predict                 predict most likely labels\r\n",
      "  predict-prob            predict most likely labels with probabilities\r\n",
      "  skipgram                train a skipgram model\r\n",
      "  cbow                    train a cbow model\r\n",
      "  print-word-vectors      print word vectors given a trained model\r\n",
      "  print-sentence-vectors  print sentence vectors given a trained model\r\n",
      "  nn                      query for nearest neighbors\r\n",
      "  analogies               query for analogies\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" 싼 게 비지떡 \". 200 ml 라고 판매 하 는데 내 가 보 기 엔 절대 이거 100 ml 도 안 된다 . 뚜껑 열 다가 손톱 부러지 고 , 향기 는 에프 킬 라 . . 구매 후기 잘 안 쓰 는데 이건 진짜 최악 이 다 . \r\n",
      "\" 비싸 ~~~~ 요 \" 라고 후기 를 남기 네요 \r\n",
      "\" 화이트 \" 로 구매 했 는데 . .. 왜 다른 제품 으로 오 죠 ? 외국 에서 배송 받 지 않 았 다면 . .. 당장 반품 인데 ~~~ \r\n",
      "\" 화이트 \" 로 구매 했 는데 . .. 다른 제품 으로 배송 되 어 왔 어요 ! ! 여기 가 외국 이 아니 라면 . .. 당장 반품 인데 ~~~ \r\n",
      "' 듀퐁 골드 ' 구매 했 는데 . . 골드 가 아니 라 X 색 이 네요 . ㅠ . ㅠ 사진 으로 만 봤 을 땐 약간 의 광택 이 있 고 화려 하 리라 생각 했 는데 . . 그렇 지 않 고 완전 텁텁 한 X 색 . .. 이사 하 고 나 서 집들이 손 님 맞이 용 으로 구매 한 거 라 교환 할 시간 도 없 고 해서 그냥 달 았 습니다 . 커튼 설치 는 신랑 이 한 거 라서 쉽 게 했 고요 . 다른 분 리플 과 는 달리 부품 은 여분 으로 많이 주 셔서 남 았 습니다 . 여분 으로 여러 개 온 것 도 모르 고 커튼 봉 에 죄다 넣 어 놔서 놀 고 있 는 부품 들 이 있 네요 . 다시 달 려고 하 니 신랑 이 귀찮 아 해서 포기 . . --; 암막 커튼 으로서 의 기능 은 확실 하 네요 . 한 낮 에 도 햇빛 하나 안 들어오 네요 . ㅋ 너무 어두워서 낮 엔 속 커튼 만 쳐 놓 고 있 어요 . 커튼 천 도 두꺼워서 바람막이 역할 도 확실히 하 네요 . 참고 로 사실 때 가로 x 세로 길 이 확실히 보 시 고 사 시 길 . . 전 가로 길이 만 생각 하 고 세로 길이 를 미처 생각 못 했 거든요 . 달 고 보 니 많이 모자라 네요 . 이런 실수 저 만 하 는 건가 . .? ^^; \r\n",
      "( NEW ) 하수구 냄새 차단 트랩 하수구 / 싱크대 / 세면대 / 세탁기 냄새 차단 ! ! 하수구 트랩 싱크대 배수구 씽크 대배 수구 냄새 는 어느 정도 차단 되 는 듯 하 나 . . 작 은 날파리 들 은 올라오 네요 -- \r\n",
      "( 지극히 개인 적 인 후기 ) 색은 차콜 로 했 지만 갈색 이 나오 는 거 같 고 이건 사람 피부색 에 따라 다를 거 같 고 저 는 괜찮 아서 색 은 참 마음 에 드 는데 아무리 유분 기 가 있 게 해서 그려 주 어도 눈썹 이 있 는 살 ( 피부 ) 만 아프 지 정말 잘 그려지 지 않 아요 ㅜㅜ제가 똥 손 이 어서 그런 거 라 생각 하 려 했 는데 아니 에요 ㅠㅠㅠㅠ진짜 그리 고 싶 어도 잘 안 나와서 못 그려요 ㅠㅠ 꾹꾹 눌러서 그려 주 면 나오 기 야 나오 는데 피부 가 너무 아파서 . .. 후 ㅠㅠ삐아 다른 색조 화장품 이나 아이라이너 쪽 은 너무 만족 해서 이것 또한 기대 하 고 구매 한 거 였 는데 그냥 갈색 색연필 로 쓰 려구요 ㅠㅠ허허 . . \r\n",
      "( 광목 패치 퍼플 . 블루 ; 화면 색상 과 달라도 너무나 다르 고 선명 하 지 않 고 빛바랜 색상 이 네요 . . 커버 두께 는 몇 번 빨 면 다 헤 질 듯이 얇 고 요즘 솜 도 압축 으로 보내 던데 통 으로 보내 서 두께 가 ;; \r\n",
      "( 행사 특가 ) 라는 표시 는 없애 시 는 게 좋 을 듯 . . 점점 가격 이 내려감 ( 행사 특가 ) 라는 표시 는 없애 시 는 게 좋 을 듯 . . 점점 가격 이 내려감 ( 행사 특가 ) 라는 표시 는 없애 시 는 게 좋 을 듯 . . 점점 가격 이 내려감 ( 행사 특가 ) 라는 표시 는 없애 시 는 게 좋 을 듯 . . 점점 가격 이 내려감 ( 행사 특가 ) 라는 표시 는 없애 시 는 게 좋 을 듯 . . 점점 가격 이 내려감 ( 행사 특가 ) 라는 표시 는 없애 시 는 게 좋 을 듯 . . 점점 가격 이 내려감 ( 행사 특가 ) 라는 표시 는 없애 시 는 게 좋 을 듯 . . 점점 가격 이 내려감 ( 행사 특가 ) 라는 표시 는 없애 시 는 게 좋 을 듯 . . 점점 가격 이 내려감 ( 행사 특가 ) 라는 표시 는 없애 시 는 게 좋 을 듯 . . 점점 가격 이 내려감 ( 행사 특가 ) 라는 표시 는 없애 시 는 게 좋 을 듯 . . 점점 가격 이 내려감 \r\n",
      "( 행사 특가 ) 페리오 치약 150 g x 20 개 청 은 차 애경 2080 송염 메디안 치석 충치 입 냄새 제거 구취 기능 성 B 상품 선택 : 45 강력 한 스 크 럽 x 4 + 촉촉 한 겔 x 4 \r\n"
     ]
    }
   ],
   "source": [
    "!head ./review.sorted.uniq.refined.tsv.text.tok\n",
    "# !head : 이런 방법으로도 head를 불러올 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  9348\n",
      "Number of labels: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005807  loss: 1.932905  eta: 0h0m .049896  loss: 2.715580  eta: 0h13m 4m   lr: 0.049188  loss: 2.364304  eta: 0h3m 1.8%  words/sec/thread: 81712  lr: 0.049119  loss: 2.360731  eta: 0h3m %  words/sec/thread: 88812  lr: 0.048894  loss: 2.350631  eta: 0h3m 2.7%  words/sec/thread: 94084  lr: 0.048674  loss: 2.343494  eta: 0h2m   loss: 2.329862  eta: 0h2m   eta: 0h2m 5.5%  words/sec/thread: 110690  lr: 0.047251  loss: 2.310515  eta: 0h2m %  words/sec/thread: 113482  lr: 0.046748  loss: 2.301699  eta: 0h2m m 116459  lr: 0.045975  loss: 2.241331  eta: 0h2m 8.1%  words/sec/thread: 116567  lr: 0.045950  loss: 2.241028  eta: 0h2m 8.3%  words/sec/thread: 116897  lr: 0.045847  loss: 2.240761  eta: 0h2m 8.5%  words/sec/thread: 117129  lr: 0.045758  loss: 2.240369  eta: 0h2m h2m   lr: 0.045514  loss: 2.235346  eta: 0h2m   eta: 0h2m 10.5%  words/sec/thread: 119731  lr: 0.044761  loss: 2.223997  eta: 0h2m   loss: 2.215080  eta: 0h2m 11.9%  words/sec/thread: 121264  lr: 0.044031  loss: 2.204809  eta: 0h2m h2m 0.043563  loss: 2.194433  eta: 0h1m 0.043390  loss: 2.194980  eta: 0h1m 122359  lr: 0.043346  loss: 2.195524  eta: 0h1m 0.042853  loss: 2.197675  eta: 0h1m 14.5%  words/sec/thread: 123151  lr: 0.042740  loss: 2.198135  eta: 0h1m   eta: 0h1m 123505  lr: 0.042353  loss: 2.198610  eta: 0h1m %  words/sec/thread: 123911  lr: 0.041970  loss: 2.199396  eta: 0h1m h1m 16.7%  words/sec/thread: 124130  lr: 0.041652  loss: 2.199644  eta: 0h1m 2.199667  eta: 0h1m   eta: 0h1m   words/sec/thread: 124341  lr: 0.041247  loss: 2.199399  eta: 0h1m 17.7%  words/sec/thread: 124426  lr: 0.041167  loss: 2.199623  eta: 0h1m 18.1%  words/sec/thread: 124565  lr: 0.040972  loss: 2.199445  eta: 0h1m   words/sec/thread: 124576  lr: 0.040955  loss: 2.199452  eta: 0h1m   words/sec/thread: 124592  lr: 0.040922  loss: 2.199290  eta: 0h1m %  words/sec/thread: 124616  lr: 0.040807  loss: 2.198778  eta: 0h1m h1m   loss: 2.193291  eta: 0h1m 19.3%  words/sec/thread: 124806  lr: 0.040342  loss: 2.181022  eta: 0h1m 19.8%  words/sec/thread: 124948  lr: 0.040124  loss: 2.179852  eta: 0h1m 20.4%  words/sec/thread: 125134  lr: 0.039777  loss: 2.178974  eta: 0h1m 0h1m 2.177176  eta: 0h1m 21.5%  words/sec/thread: 125466  lr: 0.039231  loss: 2.176647  eta: 0h1m   loss: 2.176663  eta: 0h1m %  words/sec/thread: 125684  lr: 0.038877  loss: 2.173500  eta: 0h1m %  words/sec/thread: 125708  lr: 0.038822  loss: 2.173580  eta: 0h1m 22.5%  words/sec/thread: 125745  lr: 0.038746  loss: 2.173723  eta: 0h1m 24.0%  words/sec/thread: 126180  lr: 0.038019  loss: 2.165298  eta: 0h1m   lr: 0.037569  loss: 2.158345  eta: 0h1m 25.8%  words/sec/thread: 126668  lr: 0.037118  loss: 2.159019  eta: 0h1m 26.1%  words/sec/thread: 126726  lr: 0.036972  loss: 2.159520  eta: 0h1m 26.5%  words/sec/thread: 126800  lr: 0.036767  loss: 2.159686  eta: 0h1m   lr: 0.036601  loss: 2.159918  eta: 0h1m 27.3%  words/sec/thread: 126912  lr: 0.036365  loss: 2.160254  eta: 0h1m   eta: 0h1m 127074  lr: 0.034808  loss: 2.159576  eta: 0h1m   loss: 2.157696  eta: 0h1m 31.0%  words/sec/thread: 127077  lr: 0.034502  loss: 2.150759  eta: 0h1m   words/sec/thread: 127058  lr: 0.034458  loss: 2.148255  eta: 0h1m 31.3%  words/sec/thread: 127023  lr: 0.034374  loss: 2.140413  eta: 0h1m 2.131516  eta: 0h1m 32.2%  words/sec/thread: 127087  lr: 0.033892  loss: 2.131258  eta: 0h1m 127177  lr: 0.033457  loss: 2.124644  eta: 0h1m   lr: 0.033348  loss: 2.123441  eta: 0h1m m %  words/sec/thread: 127319  lr: 0.032830  loss: 2.116160  eta: 0h1m 35.4%  words/sec/thread: 127392  lr: 0.032292  loss: 2.109345  eta: 0h1m 0h1m   lr: 0.032010  loss: 2.105588  eta: 0h1m 1m %  words/sec/thread: 127565  lr: 0.031551  loss: 2.097253  eta: 0h1m m 127660  lr: 0.031132  loss: 2.094791  eta: 0h1m 37.8%  words/sec/thread: 127659  lr: 0.031121  loss: 2.094767  eta: 0h1m 0.030888  loss: 2.093623  eta: 0h1m   lr: 0.030817  loss: 2.093157  eta: 0h1m 0.030713  loss: 2.092722  eta: 0h1m 39.0%  words/sec/thread: 127674  lr: 0.030501  loss: 2.091697  eta: 0h1m m 1m   eta: 0h1m 0h1m 42.5%  words/sec/thread: 127696  lr: 0.028733  loss: 2.083380  eta: 0h1m   words/sec/thread: 127688  lr: 0.028658  loss: 2.083020  eta: 0h1m   lr: 0.028233  loss: 2.073913  eta: 0h1m 43.6%  words/sec/thread: 127653  lr: 0.028178  loss: 2.073466  eta: 0h1m 43.8%  words/sec/thread: 127642  lr: 0.028087  loss: 2.073003  eta: 0h1m 0h1m 44.3%  words/sec/thread: 127649  lr: 0.027872  loss: 2.072206  eta: 0h1m   loss: 2.069828  eta: 0h1m 0.027201  loss: 2.069140  eta: 0h1m 127736  lr: 0.026949  loss: 2.068407  eta: 0h1m 127750  lr: 0.026569  loss: 2.065977  eta: 0h1m   loss: 2.060999  eta: 0h1m 48.5%  words/sec/thread: 127901  lr: 0.025765  loss: 2.058556  eta: 0h1m 49.4%  words/sec/thread: 127922  lr: 0.025316  loss: 2.056560  eta: 0h1m 0h1m %  words/sec/thread: 127942  lr: 0.024956  loss: 2.055533  eta: 0h1m 127911  lr: 0.024519  loss: 2.053453  eta: 0h1m 52.8%  words/sec/thread: 127906  lr: 0.023583  loss: 2.049398  eta: 0h1m   eta: 0h0m h0m h0m 0m   eta: 0h0m   lr: 0.020960  loss: 2.031378  eta: 0h0m   eta: 0h0m 58.7%  words/sec/thread: 127818  lr: 0.020653  loss: 2.029157  eta: 0h0m 58.9%  words/sec/thread: 127829  lr: 0.020567  loss: 2.028831  eta: 0h0m %  words/sec/thread: 127836  lr: 0.020490  loss: 2.028516  eta: 0h0m h0m 0.020060  loss: 2.025109  eta: 0h0m h0m 127923  lr: 0.019787  loss: 2.022705  eta: 0h0m 0.019680  loss: 2.020983  eta: 0h0m 0.019618  loss: 2.020183  eta: 0h0m 61.0%  words/sec/thread: 127925  lr: 0.019493  loss: 2.017425  eta: 0h0m   words/sec/thread: 127938  lr: 0.019200  loss: 2.013928  eta: 0h0m 62.5%  words/sec/thread: 127909  lr: 0.018769  loss: 2.008999  eta: 0h0m 127920  lr: 0.018646  loss: 2.007765  eta: 0h0m 0h0m 62.9%  words/sec/thread: 127923  lr: 0.018564  loss: 2.006987  eta: 0h0m 0.018292  loss: 2.004282  eta: 0h0m 64.7%  words/sec/thread: 127885  lr: 0.017639  loss: 1.998513  eta: 0h0m h0m   loss: 1.996144  eta: 0h0m   words/sec/thread: 127881  lr: 0.017298  loss: 1.995638  eta: 0h0m 0h0m 1.990668  eta: 0h0m   eta: 0h0m 0.015486  loss: 1.977939  eta: 0h0m 69.7%  words/sec/thread: 127710  lr: 0.015151  loss: 1.975931  eta: 0h0m 69.9%  words/sec/thread: 127701  lr: 0.015027  loss: 1.975230  eta: 0h0m m %  words/sec/thread: 127745  lr: 0.014234  loss: 1.970501  eta: 0h0m m 71.6%  words/sec/thread: 127742  lr: 0.014197  loss: 1.969715  eta: 0h0m   eta: 0h0m 0h0m 0.014008  loss: 1.968560  eta: 0h0m 1.967788  eta: 0h0m h0m %  words/sec/thread: 127762  lr: 0.013888  loss: 1.967537  eta: 0h0m   loss: 1.967306  eta: 0h0m 0.013776  loss: 1.966748  eta: 0h0m 0h0m 72.8%  words/sec/thread: 127778  lr: 0.013606  loss: 1.964681  eta: 0h0m   words/sec/thread: 127779  lr: 0.013584  loss: 1.964594  eta: 0h0m %  words/sec/thread: 127757  lr: 0.013375  loss: 1.962858  eta: 0h0m 1.962854  eta: 0h0m 73.4%  words/sec/thread: 127742  lr: 0.013279  loss: 1.962656  eta: 0h0m 0h0m   loss: 1.962195  eta: 0h0m   lr: 0.013131  loss: 1.962099  eta: 0h0m 0.013123  loss: 1.962068  eta: 0h0m m 0h0m 75.5%  words/sec/thread: 127704  lr: 0.012245  loss: 1.959026  eta: 0h0m 75.9%  words/sec/thread: 127709  lr: 0.012050  loss: 1.958380  eta: 0h0m 0.011628  loss: 1.957251  eta: 0h0m %  words/sec/thread: 127692  lr: 0.011326  loss: 1.956240  eta: 0h0m 78.3%  words/sec/thread: 127631  lr: 0.010844  loss: 1.954377  eta: 0h0m   lr: 0.010070  loss: 1.947892  eta: 0h0m h0m   lr: 0.010047  loss: 1.947751  eta: 0h0m 1.946680  eta: 0h0m 1.945827  eta: 0h0m 127569  lr: 0.009151  loss: 1.944714  eta: 0h0m 0m   lr: 0.008967  loss: 1.944261  eta: 0h0m 127600  lr: 0.008729  loss: 1.943648  eta: 0h0m   loss: 1.943339  eta: 0h0m 82.9%  words/sec/thread: 127623  lr: 0.008538  loss: 1.942452  eta: 0h0m   loss: 1.942334  eta: 0h0m 127647  lr: 0.008131  loss: 1.940968  eta: 0h0m   words/sec/thread: 127677  lr: 0.007857  loss: 1.939252  eta: 0h0m   lr: 0.007537  loss: 1.936932  eta: 0h0m 85.0%  words/sec/thread: 127686  lr: 0.007487  loss: 1.936775  eta: 0h0m %  words/sec/thread: 127665  lr: 0.006277  loss: 1.933753  eta: 0h0m 0m   lr: 0.006078  loss: 1.933376  eta: 0h0m   eta: 0h0m 0.005948  loss: 1.933139  eta: 0h0m 1.932920  eta: 0h0m 88.4%  words/sec/thread: 127656  lr: 0.005806  loss: 1.932906  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005806  loss: 1.932906  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005806  loss: 1.932907  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005806  loss: 1.932908  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005806  loss: 1.932906  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005806  loss: 1.932905  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005806  loss: 1.932906  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005806  loss: 1.932904  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005806  loss: 1.932904  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127655  lr: 0.005802  loss: 1.932901  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005802  loss: 1.932898  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127656  lr: 0.005802  loss: 1.932897  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127655  lr: 0.005802  loss: 1.932900  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127655  lr: 0.005802  loss: 1.932901  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127655  lr: 0.005802  loss: 1.932898  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127655  lr: 0.005801  loss: 1.932898  eta: 0h0m \r",
      "Progress: 88.4%  words/sec/thread: 127655  lr: 0.005801  loss: 1.932898  eta: 0h0m "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0%  words/sec/thread: 127971  lr: 0.000000  loss: 1.908565  eta: 0h0m .005727  loss: 1.932822  eta: 0h0m h0m 0m   eta: 0h0m 89.1%  words/sec/thread: 127679  lr: 0.005448  loss: 1.932159  eta: 0h0m 0h0m 1.921517  eta: 0h0m 92.5%  words/sec/thread: 127693  lr: 0.003745  loss: 1.921448  eta: 0h0m 93.3%  words/sec/thread: 127723  lr: 0.003347  loss: 1.920279  eta: 0h0m h0m   eta: 0h0m 0h0m   loss: 1.917798  eta: 0h0m   eta: 0h0m 0m 127876  lr: 0.002059  loss: 1.915158  eta: 0h0m 0h0m 98.5%  words/sec/thread: 127954  lr: 0.000729  loss: 1.909921  eta: 0h0m   words/sec/thread: 127958  lr: 0.000628  loss: 1.909728  eta: 0h0m \n"
     ]
    }
   ],
   "source": [
    "!fasttext skipgram -input ./review.sorted.uniq.refined.tsv.text.tok -output ko.tok -dim 256 -epoch 100 -minCount 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ko.tok.bin : 뉴럴네트워크의 결과가 있는 곳 \n",
    "2. ko.tok.vec : 벡터가 들어있는 곳 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing word vectors... done.\n",
      "Query word? 귀찮 0.735655\n",
      "그냥 0.642696\n",
      "교환 0.633681\n",
      "환불 0.515049\n",
      "걍 0.438973\n",
      "씁니다 0.436516\n",
      "려다 0.436276\n",
      "다시 0.414777\n",
      "신청 0.406632\n",
      "반송 0.401465\n",
      "Query word? "
     ]
    }
   ],
   "source": [
    "!echo '반품' | fasttext nn ./ko.tok.bin 10\n",
    "# 반품에 대해 nearest neighbor 10개 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9348 256\r\n",
      ". 0.099576 -0.081921 -0.063062 -0.07576 0.020853 -0.036874 -0.061276 0.060236 0.049235 0.10304 0.094375 -0.072827 -0.022418 -0.072007 0.023674 0.034476 -0.084638 -0.03798 -0.16071 0.024266 0.025342 -0.081887 -0.09408 0.037213 -0.1698 0.13684 -0.11353 -0.01392 0.036803 0.010171 0.079484 -0.021137 0.11858 -0.050828 -0.060875 -0.10583 -0.00088811 -0.072351 0.11429 0.027349 0.083469 0.058815 -0.15096 0.012911 -0.020363 0.042089 -0.082265 -0.024705 -0.07074 0.011057 -0.058104 -0.04872 -0.010493 0.0045092 -0.0039106 -0.10217 0.014542 0.032405 -0.021974 -0.063069 -0.12137 -0.033627 -0.036507 0.027569 -0.19755 -0.0010203 0.07629 0.077576 -0.060431 0.051735 0.094824 0.0041292 0.042246 0.13798 -0.057463 0.10897 0.090754 -0.021485 0.02871 -0.035706 -0.059248 0.04699 -0.0057002 -0.03806 -0.064533 -0.11594 0.12541 0.020665 -0.069997 -0.12836 -0.17248 0.01411 0.10339 -0.021588 0.019771 0.014128 0.058918 0.0051285 -0.15562 -0.013094 -0.036499 -0.0824 -0.069976 -0.033404 -0.026553 0.07043 0.051972 0.026272 -0.16139 -0.10101 0.15032 -0.10606 0.014491 0.068 0.096919 -0.069375 0.064682 0.064605 -0.064871 -0.0088585 -0.07222 0.11304 0.025758 -0.024881 0.071485 0.10765 0.11864 0.074256 0.01506 0.033447 0.010175 -0.026817 0.11597 -0.0040474 0.05173 -0.034862 0.12747 -0.046874 -0.057725 0.11729 -0.028626 0.10686 0.059916 -0.011545 0.056655 0.020613 -0.02129 0.0063178 -0.024883 0.086805 -0.026174 -0.020075 0.070703 -0.030146 -0.017355 -0.021339 -0.060791 0.0087313 0.16095 0.10382 0.097882 -0.0047578 -0.15801 -0.069256 0.039639 -0.16094 0.033552 -0.0080713 0.064136 -0.0070595 -0.080757 0.071747 0.075066 -0.083204 -0.063177 -0.14955 0.13158 0.15621 -0.065896 0.15969 0.081607 0.052582 0.064934 0.07565 0.087303 -0.072258 -0.04189 0.11903 -0.033492 -0.032924 -0.043044 -0.034406 -0.0063043 -0.13554 -0.12235 -0.0071524 0.0081905 -0.092034 -0.054032 -0.15857 0.041632 0.040489 0.10049 0.073373 -0.020738 0.094542 0.055728 0.037513 -0.16637 -0.15516 0.061124 -0.038488 0.085079 0.061374 -0.098677 -0.035276 -0.054058 0.044561 0.054405 0.039028 0.078532 -0.13188 0.17108 -0.0064657 0.031758 -0.139 0.060278 -0.14426 -0.096594 0.095546 0.0074933 0.021775 -0.1588 0.016501 -0.075454 0.031833 0.042417 0.050649 -0.15907 0.0094118 -0.086223 0.11942 0.15894 0.01269 -0.0056038 0.086537 -0.033215 0.11806 -0.024056 -0.0086749 0.06409 0.053181 -0.067129 0.001501 -0.07424 -0.036433 \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 ./ko.tok.vec\n",
    "# 9348개의 워드 임베딩 벡터가 있고 각각 256차원 \n",
    "# 굉장히 Dense함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
